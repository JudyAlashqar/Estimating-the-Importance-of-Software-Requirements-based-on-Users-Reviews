{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "_YMCxxEEfcdb",
        "CfKdj6ACfezb",
        "NLHdPxIpfnrX",
        "V5gvB66EfuP0",
        "QlLz58psf3je",
        "xMNsJg-WgYTb",
        "R728nE2jgg0o",
        "PJl2iWKNg1YT",
        "Dq0p1CEug320",
        "WDvnLJwAlUDW",
        "wiJXxLilloXw",
        "MA8S0OukmBXk",
        "bPPkAy7dmEjv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries"
      ],
      "metadata": {
        "id": "_YMCxxEEfcdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_diX-dWqWnN",
        "outputId": "a9b09d4c-9a47-4024-b693-31f8f7247e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "!pip install stanza\n",
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "V7iYe9fVeMZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "CfKdj6ACfezb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import mplcursors\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import stanza\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import fasttext\n",
        "from tensorflow.keras.models import load_model\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "x8GLkupsegqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Models from Previous Phase"
      ],
      "metadata": {
        "id": "NLHdPxIpfnrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model = fasttext.load_model(\"/gdrive/MyDrive/Project/train_100_single_epoch50.bin\")\n",
        "filter_model = load_model(\"/gdrive/MyDrive/Project/FastTextFilterModel.bin\")\n",
        "classification_model = load_model(\"/gdrive/MyDrive/Project/FastTextClassificationModel.bin\")\n",
        "sentiment_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")"
      ],
      "metadata": {
        "id": "inc_TUY19PPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "V5gvB66EfuP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(data):\n",
        "  nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
        "  tokenized_data = []\n",
        "  for i in range(0, len(data)):\n",
        "    doc = data[i]\n",
        "    doc = nlp(doc)\n",
        "    doc = [str(token.text) for sent in doc.sentences for token in sent.tokens]\n",
        "    doc = ' '.join(doc)\n",
        "    tokenized_data.append(doc)\n",
        "  return tokenized_data"
      ],
      "metadata": {
        "id": "h7jnciG_JpKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def whitespace_tokenizer(sent):\n",
        "  return sent.split()"
      ],
      "metadata": {
        "id": "ONbnnOQP99jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_tags(data):\n",
        "  nlp = stanza.Pipeline(lang='en', processors='pos, tokenize')\n",
        "  POS_tags = []\n",
        "  for i in range(0, len(data)):\n",
        "    doc = data[i]\n",
        "    doc = nlp(doc)\n",
        "    tags= [str(word.pos) for sent in doc.sentences for word in sent.words]\n",
        "    POS_tags.append(tags)\n",
        "  return POS_tags"
      ],
      "metadata": {
        "id": "6gHdhXus3lhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_reviews(reviews):\n",
        "  predictions = filter_model.predict(get_fastText_embedding(reviews))\n",
        "  filtered_reviews = []\n",
        "  for prediction, review in zip(predictions, reviews):\n",
        "    if(np.argmax(prediction) == 1):\n",
        "      filtered_reviews.append(review)\n",
        "  return filtered_reviews"
      ],
      "metadata": {
        "id": "4385kjzM9aYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "QlLz58psf3je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_weights(POS_tags, weights = [1, 1, 0]):\n",
        "  weights = []\n",
        "  for sentence_tags in POS_tags:\n",
        "    sentence_weights = []\n",
        "    for tag in sentence_tags:\n",
        "      if(tag == \"VERB\"):\n",
        "        sentence_weights.append(weights[0])\n",
        "      elif(tag == \"NOUN\"):\n",
        "        sentence_weights.append(weights[1])\n",
        "      else:\n",
        "        sentence_weights.append(weights[2])\n",
        "    weights.append(sentence_weights)\n",
        "  return weights"
      ],
      "metadata": {
        "id": "yFTFU8kg3lE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(A,B):\n",
        "  ans = np.dot(A,B)/(norm(A)*norm(B))\n",
        "  return ans"
      ],
      "metadata": {
        "id": "OKui_L0lnxwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_requirement_reviews(reviews, requirements, threshold = 0.75):\n",
        "  tokenized_reviews = tokenize_data(reviews)\n",
        "  tokenized_requirements = tokenize_data(requirements)\n",
        "  reviews_pos_tags = get_pos_tags(tokenized_reviews)\n",
        "  req_pos_tags = get_pos_tags(tokenized_requirements)\n",
        "  reviews_weights = get_pos_weights(reviews_pos_tags)\n",
        "  req_weights = get_pos_weights(req_pos_tags)\n",
        "  app_reviews_embeddings = get_pos_weighted_fastText_embedding(tokenized_reviews, reviews_weights)\n",
        "  app_requirements_embeddings = get_pos_weighted_fastText_embedding(tokenized_requirements, req_weights)\n",
        "  req_review_dict = {}\n",
        "  for i, review in enumerate(app_reviews_embeddings):\n",
        "    for j, req in enumerate(app_requirements_embeddings):\n",
        "      if i >= j:\n",
        "        continue\n",
        "      similarity = cosine_similarity(review, req)\n",
        "      if(similarity > threshold):\n",
        "        if(requirements[j] not in req_review_dict.keys()):\n",
        "          req_review_dict.update({requirements[j] : [reviews[i]]})\n",
        "        else:\n",
        "          req_review_dict[requirements[j]].append(reviews[i])\n",
        "  return req_review_dict"
      ],
      "metadata": {
        "id": "_Z-dT7K25bkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_requirements(req_review_dict):\n",
        "  req_types_dict = {}\n",
        "  for item in req_review_dict.items():\n",
        "    predictions = classification_model.predict(get_fastText_embedding(item[1]))\n",
        "    prediction_labels = []\n",
        "    for prediction in predictions:\n",
        "      prediction_labels.append(np.argmax(prediction))\n",
        "    #FR:0, PD:1, UE:2\n",
        "    if(1 in prediction_labels):\n",
        "      req_types_dict.update({item[0] :\"PD\"})\n",
        "    elif(0 in prediction_labels):\n",
        "      req_types_dict.update({item[0] :\"FR\"})\n",
        "    else:\n",
        "      req_types_dict.update({item[0] :\"Other\"})\n",
        "  return req_types_dict"
      ],
      "metadata": {
        "id": "4RSov7j-_G27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_reviews(reviews):\n",
        "    rev_types_dict = {}\n",
        "    predictions = classification_model.predict(get_fastText_embedding(reviews))\n",
        "    prediction_labels = []\n",
        "    for prediction in predictions:\n",
        "      prediction_labels.append(np.argmax(prediction))\n",
        "    for review, prediction in zip(reviews, prediction_labels):\n",
        "      #FR:0, PD:1, UE:2\n",
        "      if prediction == 0:\n",
        "        rev_types_dict.update({review : \"Feature Request\"})\n",
        "      elif prediction == 1:\n",
        "        rev_types_dict.update({review : \"Bug Report\"})\n",
        "      else:\n",
        "        rev_types_dict.update({review :\"Information Seeking or Giving\"})\n",
        "    return rev_types_dict"
      ],
      "metadata": {
        "id": "y0EXSVnYjKL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_requirements_sentiments_score(req_review_dict):\n",
        "  req_sent_dict = {}\n",
        "  for item in req_review_dict.items():\n",
        "    results = sentiment_model(item[1])\n",
        "    sentiment = 0\n",
        "    for result in results:\n",
        "      if(result['label'] == 'POS'):\n",
        "        sentiment += result['score'] * 0.5\n",
        "      elif(result['label'] == 'NEG'):\n",
        "        sentiment += result['score']\n",
        "    req_sent_dict.update({item[0] : sentiment/len(item[1])})\n",
        "  return req_sent_dict"
      ],
      "metadata": {
        "id": "-lCwKZxiBZLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_importance(req_review_dict, req_types_dict, req_sent_dict):\n",
        "  req_importance_dict = {}\n",
        "  max_mentions = 0\n",
        "  for item in req_review_dict.items():\n",
        "    if(len(item[1]) > max_mentions):\n",
        "      max_mentions = len(item[1])\n",
        "  for item in req_review_dict.items():\n",
        "    score = 0\n",
        "    score += len(item[1])/max_mentions\n",
        "    score += req_sent_dict[item[0]]\n",
        "    if(req_types_dict[item[0]] == \"FR\"):\n",
        "      score += 0.5\n",
        "    elif (req_types_dict[item[0]] == \"PD\"):\n",
        "      score += 1\n",
        "    else:\n",
        "      score += 0.25\n",
        "    req_importance_dict.update({item[0] : score })\n",
        "  return req_importance_dict"
      ],
      "metadata": {
        "id": "lCV5yXL4Dz_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_requirements_sorted(req_importance_dict):\n",
        "  sorted_req = sorted(req_importance_dict.items(),key=lambda x:x[1],reverse=True)\n",
        "  for item in sorted_req:\n",
        "    print(item[0], item[1])\n",
        "    print(\"----------------------------\")"
      ],
      "metadata": {
        "id": "MxOnpG1xdG8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Representaion"
      ],
      "metadata": {
        "id": "xMNsJg-WgYTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fastText_embedding(data):\n",
        "  embeddings = np.zeros(shape=(len(data), ft_model.get_dimension()), dtype = 'float32')\n",
        "  for i, review in enumerate(data):\n",
        "    review_embedding = np.zeros(shape=(ft_model.get_dimension(),), dtype = 'float32')\n",
        "    words_count = 0\n",
        "    for word in review.lower().split():\n",
        "      words_count = words_count + 1\n",
        "      word_embedding = ft_model.get_word_vector(word).astype('float32')\n",
        "      review_embedding = review_embedding + word_embedding\n",
        "    review_embedding = review_embedding/words_count\n",
        "    embeddings[i] = review_embedding\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "T2JT3kRkF6xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_weighted_fastText_embedding(data, weights):\n",
        "  embeddings = np.zeros(shape=(len(data), ft_model.get_dimension()), dtype = 'float32')\n",
        "  for i, review in enumerate(data):\n",
        "    review_embedding = np.zeros(shape=(ft_model.get_dimension(),), dtype = 'float32')\n",
        "    weights_sum = 0\n",
        "    for j, word in enumerate(review.split()):\n",
        "      weights_sum = weights_sum + weights[i][j]\n",
        "      word_embedding = ft_model.get_word_vector(word).astype('float32') * weights[i][j]\n",
        "      review_embedding = review_embedding + word_embedding\n",
        "    if(weights_sum != 0):\n",
        "      review_embedding = review_embedding/weights_sum\n",
        "    embeddings[i] = review_embedding\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "BX5zToToDjgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement The Phase"
      ],
      "metadata": {
        "id": "R728nE2jgg0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "PJl2iWKNg1YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Reviews, Requirements files\n",
        "app_name = \"Messenger\"\n",
        "file = open(f'{app_name}_reviews.txt', \"r\")\n",
        "data = file.read()\n",
        "reviews = data.split(\"\\n\")\n",
        "file = open(f'{app_name}_requirements.txt', \"r\")\n",
        "data = file.read()\n",
        "requirements = data.split(\"\\n\")\n",
        "filtered_reviews = filter_reviews(reviews)\n",
        "review_types_dict = classify_reviews(filtered_reviews)\n",
        "\n",
        "# find Feature Request Reviews that are not matched in the previous phase\n",
        "discussed_reviews = []\n",
        "for item in req_review_dict.items():\n",
        "  for review in item[1]:\n",
        "    discussed_reviews.append(review)\n",
        "fr_reviews = [review for review in reviews if review not in unique(discussed_reviews) and review_types_dict[review] == \"Feature Request\" ]\n",
        "fr_embeddings = get_fastText_embedding(fr_reviews)"
      ],
      "metadata": {
        "id": "komyS-wGnrjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimate Importance"
      ],
      "metadata": {
        "id": "Dq0p1CEug320"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "req_review_dict = match_requirement_reviews(filtered_reviews, requirements)\n",
        "req_types_dict = classify_requirements(req_review_dict)\n",
        "req_sent_dict = get_requirements_sentiments_score(req_review_dict)\n",
        "req_importance_dict = estimate_importance(req_review_dict, req_types_dict, req_sent_dict)\n",
        "print_requirements_sorted(req_importance_dict)"
      ],
      "metadata": {
        "id": "v9ouePtVvuk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suggesting New Requirements"
      ],
      "metadata": {
        "id": "WDvnLJwAlUDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement"
      ],
      "metadata": {
        "id": "wiJXxLilloXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 5, random_state=0).fit(fr_embeddings)\n",
        "new_groups = []\n",
        "for i in range(5):\n",
        "  group = []\n",
        "  for review, label in zip(fr_reviews, kmeans.labels_):\n",
        "    if(label == i):\n",
        "      group.append(review)\n",
        "  new_groups.append(group)\n",
        "groups_keywords = []\n",
        "for group in new_groups:\n",
        "  keywords ={}\n",
        "  for item in group:\n",
        "    print(item)\n",
        "    words = item.split()\n",
        "    for word in words:\n",
        "      if(word not in keywords.keys()):\n",
        "        if (word not in stopwords):\n",
        "          keywords.update({word : 1})\n",
        "      else:\n",
        "        keywords.update({word : keywords[word] + 1})\n",
        "  keywords = sorted(keywords.items(),key=lambda x:x[1],reverse=True)\n",
        "  keywords = [keyword[0] for keyword in keywords]\n",
        "  groups_keywords.append(keywords[:3])\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(fr_embeddings)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['dim1', 'dim2'])\n",
        "finalDf = pd.concat([principalDf, pd.DataFrame(kmeans.labels_, columns =[\"label\"])], axis = 1)"
      ],
      "metadata": {
        "id": "lb4EP6lai7Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print Suggested Requirements Keywords"
      ],
      "metadata": {
        "id": "MA8S0OukmBXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Suggested Requirements Keywords\")\n",
        "for group in groups_keywords:\n",
        "  print(group)\n",
        "  print(\"--------------\")"
      ],
      "metadata": {
        "id": "SvgloK2plAsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Suggested Requirements Clusters"
      ],
      "metadata": {
        "id": "bPPkAy7dmEjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(fr_embeddings)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])\n",
        "finalDf = pd.concat([principalDf, pd.DataFrame(kmeans.labels_, columns =[\"label\"])], axis = 1)\n",
        "\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "\n",
        "targets = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "colors = ['r', 'g', 'b', 'c', 'm', 'y' ,'k', '#aaaaaa']\n",
        "for target, color in zip(targets,colors):\n",
        "    indicesToKeep = finalDf['label'] == target\n",
        "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "metadata": {
        "id": "jQl1k1Rgkzmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}