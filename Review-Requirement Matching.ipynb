{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "zMo7cDa6aXQz",
        "B6l3UtSKaeBs",
        "atkcjhRtai3K",
        "lHY04qi5anSF",
        "HQopbUe4aqz7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries"
      ],
      "metadata": {
        "id": "zMo7cDa6aXQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_diX-dWqWnN",
        "outputId": "9f3c3f1d-0884-477d-c688-69e7e0568113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "!pip install stanza"
      ],
      "metadata": {
        "id": "H2qCEp47UNTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "B6l3UtSKaeBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from  nltk.tokenize  import word_tokenize\n",
        "import stanza\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import fasttext\n",
        "from tensorflow.keras.models import load_model\n",
        "import json\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "H3jgdhS5U6vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Models from Previous Phase"
      ],
      "metadata": {
        "id": "atkcjhRtai3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model = fasttext.load_model(\"/gdrive/MyDrive/Project/train_100_single_epoch50.bin\")\n",
        "filter_model = load_model(\"/gdrive/MyDrive/Project/FilterModel.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tJTdb8aDlmU",
        "outputId": "0669c51f-4e67-48df-eca9-0389e076dabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "lHY04qi5anSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(data):\n",
        "  nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
        "  tokenized_data = []\n",
        "  for i in range(0, len(data)):\n",
        "    doc = data[i]\n",
        "    doc = nlp(doc)\n",
        "    doc = [str(token.text) for sent in doc.sentences for token in sent.tokens]\n",
        "    doc = ' '.join(doc)\n",
        "    tokenized_data.append(doc)\n",
        "  return tokenized_data"
      ],
      "metadata": {
        "id": "h7jnciG_JpKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def whitespace_tokenizer(sent):\n",
        "  return sent.split()"
      ],
      "metadata": {
        "id": "ONbnnOQP99jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_tags(data):\n",
        "  nlp = stanza.Pipeline(lang='en', processors='pos, tokenize')\n",
        "  POS_tags = []\n",
        "  for i in range(0, len(data)):\n",
        "    doc = data[i]\n",
        "    doc = nlp(doc)\n",
        "    tags= [str(word.pos) for sent in doc.sentences for word in sent.words]\n",
        "    POS_tags.append(tags)\n",
        "  return POS_tags"
      ],
      "metadata": {
        "id": "6gHdhXus3lhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stanza_tokenizer(doc):\n",
        "  nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
        "  doc = nlp(doc)\n",
        "  return [str(token.text) for sent in doc.sentences for token in sent.tokens]"
      ],
      "metadata": {
        "id": "NqP0RrMyUiAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_reviews(reviews):\n",
        "  predictions = filter_model.predict(get_fastText_embedding(reviews))\n",
        "  filtered_reviews = []\n",
        "  for prediction, review in zip(predictions, reviews):\n",
        "    if(np.argmax(prediction) == 1):\n",
        "      filtered_reviews.append(review)\n",
        "  return filtered_reviews"
      ],
      "metadata": {
        "id": "4385kjzM9aYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "HQopbUe4aqz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_idf(processed_data):\n",
        "    count_vectorizer = CountVectorizer(tokenizer=whitespace_tokenizer, token_pattern = None, lowercase=False)\n",
        "    count_vectorizer.fit(processed_data)\n",
        "    tf_matrix = count_vectorizer.transform(processed_data)\n",
        "    doc_freq = np.array(tf_matrix.astype(bool).sum(axis=0)).flatten()\n",
        "    idf = np.log(len(processed_data) / (doc_freq))\n",
        "    return idf.tolist(), count_vectorizer"
      ],
      "metadata": {
        "id": "7ji-urJFP93g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_weights(POS_tags, weights = [1, 1, 0.5]):\n",
        "  weights = []\n",
        "  for sentence_tags in POS_tags:\n",
        "    sentence_weights = []\n",
        "    for tag in sentence_tags:\n",
        "      if(tag == \"VERB\"):\n",
        "        sentence_weights.append(weights[0])\n",
        "      elif(tag == \"NOUN\"):\n",
        "        sentence_weights.append(weights[1])\n",
        "      else:\n",
        "        sentence_weights.append(weights[2])\n",
        "    weights.append(sentence_weights)\n",
        "  return weights"
      ],
      "metadata": {
        "id": "yFTFU8kg3lE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(A,B):\n",
        "  ans = np.dot(A,B)/(norm(A)*norm(B))\n",
        "  return ans"
      ],
      "metadata": {
        "id": "OKui_L0lnxwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Union(lst1, lst2):\n",
        "  final_list = list(set(lst1) | set(lst2))\n",
        "  return final_list"
      ],
      "metadata": {
        "id": "EVSOr1ldXPKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unique(list1):\n",
        "  unique_list = []\n",
        "  for x in list1:\n",
        "      if x not in unique_list:\n",
        "          unique_list.append(x)\n",
        "  return unique_list"
      ],
      "metadata": {
        "id": "ZYYGzWb5HXAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Representaion"
      ],
      "metadata": {
        "id": "0VHtKobFauz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_weighted_fastText_embedding(data, weights):\n",
        "  embeddings = np.zeros(shape=(len(data), ft_model.get_dimension()), dtype = 'float32')\n",
        "  for i, review in enumerate(data):\n",
        "    review_embedding = np.zeros(shape=(ft_model.get_dimension(),), dtype = 'float32')\n",
        "    weights_sum = 0\n",
        "    for j, word in enumerate(review.split()):\n",
        "      weights_sum = weights_sum + weights[i][j]\n",
        "      word_embedding = ft_model.get_word_vector(word).astype('float32') * weights[i][j]\n",
        "      review_embedding = review_embedding + word_embedding\n",
        "    if(weights_sum != 0):\n",
        "      review_embedding = review_embedding/weights_sum\n",
        "    embeddings[i] = review_embedding\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "BX5zToToDjgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement The Phase"
      ],
      "metadata": {
        "id": "73uOTezcbDD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "0TioobkudGBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Reviews, Requirements and Annotation files\n",
        "apps_names = {\"Messenger\"}\n",
        "apps_reviews = {}\n",
        "for app_name in apps_names:\n",
        "  file = open(f'{app_name}_reviews.txt', \"r\")\n",
        "  data = file.read()\n",
        "  reviews = data.split(\"\\n\")\n",
        "  reviews = filter_reviews(reviews)\n",
        "  apps_reviews.update({app_name: reviews})\n",
        "\n",
        "apps_requirements = {}\n",
        "for app_name in apps_names:\n",
        "  file = open(f'{app_name}_requirements.txt', \"r\")\n",
        "  data = file.read()\n",
        "  requirements = data.split(\"\\n\")\n",
        "  apps_requirements.update({app_name: requirements})\n",
        "\n",
        "with open('annotation.txt') as json_file:\n",
        "    annot = json.load(json_file)\n",
        "\n",
        "# Get Reviews and Requirements Representaions\n",
        "reviews_pos_tags = get_pos_tags(tokenized_messenger_reviews)\n",
        "req_pos_tags = get_pos_tags(tokenized_messenger_requirements)\n",
        "reviews_weights = get_pos_weights(reviews_pos_tags)\n",
        "req_weights = get_pos_weights(req_pos_tags)\n",
        "apps_reviews_embeddings = {}\n",
        "apps_requirements_embeddings = {}\n",
        "for app_name in apps_names:\n",
        "  app_reviews_embeddings = get_pos_weighted_fastText_embedding(tokenized_messenger_reviews, reviews_weights)\n",
        "  app_requirements_embeddings = get_pos_weighted_fastText_embedding(tokenized_messenger_requirements, req_weights)\n",
        "  apps_reviews_embeddings.update({app_name: app_reviews_embeddings})\n",
        "  apps_requirements_embeddings.update({app_name: app_requirements_embeddings})"
      ],
      "metadata": {
        "id": "komyS-wGnrjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expirements"
      ],
      "metadata": {
        "id": "wPUsNZOTdPSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different matching-thresholds and evaluate precision, recall, f1-measure, f2-measure\n",
        "thresholds = np.arange(0,1.05,0.05)\n",
        "p_results = []\n",
        "r_results = []\n",
        "f1_results = []\n",
        "f2_results = []\n",
        "for th in thresholds:\n",
        "  req_rev_matches = {}\n",
        "  for app_name in apps_names:\n",
        "    for item in sorted_apps[app_name]:\n",
        "      if (item[1] < th):\n",
        "        break\n",
        "      if(item[0][0] not in req_rev_matches.keys()):\n",
        "        req_rev_matches.update({item[0][0]:[item[0][1]]})\n",
        "      else:\n",
        "        req_rev_matches[item[0][0]].append(item[0][1])\n",
        "  tp = 0\n",
        "  tn = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "  for review in apps_reviews[\"Messenger\"]:\n",
        "    if(review in req_rev_matches):\n",
        "      list3 = req_rev_matches[review]\n",
        "    else:\n",
        "      list3 = []\n",
        "    list1 = annot[review][0]\n",
        "    list2 = annot[review][1]\n",
        "    lis = list1 + list2\n",
        "    for elem in list3:\n",
        "      if elem in lis:\n",
        "        tp += 1\n",
        "      if (elem not in lis):\n",
        "        fp+=1\n",
        "    for elem in lis:\n",
        "      if (elem not in list3):\n",
        "        fn+=1\n",
        "    tn = tn + (70 - len(Union(lis, list3)))\n",
        "  precision = float('inf')\n",
        "  recall = float('inf')\n",
        "  f1 = float('inf')\n",
        "  f2 = float('inf')\n",
        "  if((tp + fp) != 0):\n",
        "    precision = tp/(tp + fp)\n",
        "  if(tp + fn):\n",
        "    recall = tp/(tp + fn)\n",
        "  if(precision+recall):\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "  if(4*precision + recall):\n",
        "    f2 = 5*precision*recall/(4*precision + recall)\n",
        "  p_results.append(precision)\n",
        "  r_results.append(recall)\n",
        "  f1_results.append(f1)\n",
        "  f2_results.append(f2)"
      ],
      "metadata": {
        "id": "vQPpx7kXdNdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the Results"
      ],
      "metadata": {
        "id": "cFw8iX75dSMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(thresholds, p_results)\n",
        "plt.plot(thresholds, r_results)\n",
        "plt.plot(thresholds, f2_results)\n",
        "plt.legend([\"Precesion\",\"Recall\",\"F2-measure\"])\n",
        "plt.xlabel(\"threshold\")"
      ],
      "metadata": {
        "id": "Y5wf4YS3Yuhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(r_results,p_results,'-o')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.xticks(np.arange(0,1.1,0.1))\n",
        "plt.yticks(np.arange(0,1.1,0.1))"
      ],
      "metadata": {
        "id": "gTnYLgDJc7Vp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}