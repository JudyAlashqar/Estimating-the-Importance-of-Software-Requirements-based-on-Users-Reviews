{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAyr552N8A5U"
      },
      "source": [
        "# Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgTeqVgKi5nB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_HF4l9MjBDh"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext\n",
        "!pip install -q -U \"tensorflow-text==2.11.*\"\n",
        "!pip install stanza\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsDyh0iJ77n_"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvDIHbgbizI6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import keras\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "import nltk\n",
        "from  nltk.tokenize  import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import stanza\n",
        "import contractions\n",
        "import fasttext\n",
        "import fasttext.util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K3RfxwFloib"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "fasttext.util.download_model('en', if_exists='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIjGivHnwd71"
      },
      "source": [
        "# Data Preparing and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNb99W2KHR4u"
      },
      "outputs": [],
      "source": [
        "def read_data(path, columns_list):\n",
        "  dataset = pd.read_excel(path)\n",
        "  dataset = dataset[columns_list]\n",
        "  dataset.head()\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72Z3OfAnHTeL"
      },
      "outputs": [],
      "source": [
        "def prepare_data(dataset, features_list, target, target_classes):\n",
        "  for index, record in dataset.iterrows():\n",
        "    for i in range(len(target_classes)):\n",
        "      if(record[target] == target_classes[i]):\n",
        "        record[target] = i\n",
        "  X = dataset[features_list[0]].tolist()\n",
        "  y = dataset[target].tolist()\n",
        "  return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLlfwCUUHYc4"
      },
      "outputs": [],
      "source": [
        "def get_one_hot_vectors(labels):\n",
        "  one_hot_labels = []\n",
        "  for label in labels:\n",
        "    for i in unique(labels):\n",
        "      if (label == i):\n",
        "        lis = len(unique(labels)) * [0]\n",
        "        lis[label] = 1\n",
        "        one_hot_labels.append(lis)\n",
        "  return one_hot_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pecuH3bkUaHm"
      },
      "outputs": [],
      "source": [
        "def whitespace_tokenizer(sent):\n",
        "  return sent.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7jnciG_JpKI"
      },
      "outputs": [],
      "source": [
        "def tokenize_data(data):\n",
        "  nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
        "  tokenized_data = []\n",
        "  for i in range(0, len(data)):\n",
        "    doc = data[i]\n",
        "    doc = nlp(doc)\n",
        "    doc = [str(token.text) for sent in doc.sentences for token in sent.tokens]\n",
        "    doc = ' '.join(doc)\n",
        "    tokenized_data.append(doc)\n",
        "  return tokenized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTOzqkgvPHjX"
      },
      "outputs": [],
      "source": [
        "def expand_data(data):\n",
        "  expanded_text = []\n",
        "  for sent in data:\n",
        "    expanded_words = []\n",
        "    for word in sent.split():\n",
        "      expanded_words.append(contractions.fix(word))\n",
        "    expanded_text.append(' '.join(expanded_words))\n",
        "  return(expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysAOgv-EF_78"
      },
      "outputs": [],
      "source": [
        "def get_pos_tags(data):\n",
        "  nlp = stanza.Pipeline(lang='en', processors='pos, tokenize')\n",
        "  POS_tags = []\n",
        "  for i in range(0, len(data)):\n",
        "    doc = data[i]\n",
        "    doc = nlp(doc)\n",
        "    tags= [str(word.pos) for sent in doc.sentences for word in sent.words]\n",
        "    POS_tags.append(tags)\n",
        "  return POS_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BovwJeFkd0_"
      },
      "outputs": [],
      "source": [
        "def slang_transformer(reviews):\n",
        "  abbreviations = {\n",
        "    \"$\" : \" dollar \",\n",
        "    \"â‚¬\" : \" euro \",\n",
        "    \"4ao\" : \"for adults only\",\n",
        "    \"a.m\" : \"before midday\",\n",
        "    \"a3\" : \"anytime anywhere anyplace\",\n",
        "    \"aamof\" : \"as a matter of fact\",\n",
        "    \"acct\" : \"account\",\n",
        "    \"adih\" : \"another day in hell\",\n",
        "    \"afaic\" : \"as far as i am concerned\",\n",
        "    \"afaict\" : \"as far as i can tell\",\n",
        "    \"afaik\" : \"as far as i know\",\n",
        "    \"afair\" : \"as far as i remember\",\n",
        "    \"afk\" : \"away from keyboard\",\n",
        "    \"app\" : \"application\",\n",
        "    \"approx\" : \"approximately\",\n",
        "    \"apps\" : \"applications\",\n",
        "    \"asap\" : \"as soon as possible\",\n",
        "    \"asl\" : \"age, sex, location\",\n",
        "    \"atk\" : \"at the keyboard\",\n",
        "    \"ave.\" : \"avenue\",\n",
        "    \"aymm\" : \"are you my mother\",\n",
        "    \"ayor\" : \"at your own risk\",\n",
        "    \"b&b\" : \"bed and breakfast\",\n",
        "    \"b+b\" : \"bed and breakfast\",\n",
        "    \"b.c\" : \"before christ\",\n",
        "    \"b2b\" : \"business to business\",\n",
        "    \"b2c\" : \"business to customer\",\n",
        "    \"b4\" : \"before\",\n",
        "    \"b4n\" : \"bye for now\",\n",
        "    \"b@u\" : \"back at you\",\n",
        "    \"bae\" : \"before anyone else\",\n",
        "    \"bak\" : \"back at keyboard\",\n",
        "    \"bbbg\" : \"bye bye be good\",\n",
        "    \"bbc\" : \"british broadcasting corporation\",\n",
        "    \"bbias\" : \"be back in a second\",\n",
        "    \"bbl\" : \"be back later\",\n",
        "    \"bbs\" : \"be back soon\",\n",
        "    \"be4\" : \"before\",\n",
        "    \"bfn\" : \"bye for now\",\n",
        "    \"blvd\" : \"boulevard\",\n",
        "    \"bout\" : \"about\",\n",
        "    \"brb\" : \"be right back\",\n",
        "    \"bros\" : \"brothers\",\n",
        "    \"brt\" : \"be right there\",\n",
        "    \"bsaaw\" : \"big smile and a wink\",\n",
        "    \"btw\" : \"by the way\",\n",
        "    \"bwl\" : \"bursting with laughter\",\n",
        "    \"c/o\" : \"care of\",\n",
        "    \"cet\" : \"central european time\",\n",
        "    \"cf\" : \"compare\",\n",
        "    \"cia\" : \"central intelligence agency\",\n",
        "    \"csl\" : \"can not stop laughing\",\n",
        "    \"cu\" : \"see you\",\n",
        "    \"cul8r\" : \"see you later\",\n",
        "    \"cv\" : \"curriculum vitae\",\n",
        "    \"cwot\" : \"complete waste of time\",\n",
        "    \"cya\" : \"see you\",\n",
        "    \"cyt\" : \"see you tomorrow\",\n",
        "    \"dae\" : \"does anyone else\",\n",
        "    \"dbmib\" : \"do not bother me i am busy\",\n",
        "    \"diy\" : \"do it yourself\",\n",
        "    \"dm\" : \"direct message\",\n",
        "    \"dwh\" : \"during work hours\",\n",
        "    \"e123\" : \"easy as one two three\",\n",
        "    \"eet\" : \"eastern european time\",\n",
        "    \"eg\" : \"example\",\n",
        "    \"embm\" : \"early morning business meeting\",\n",
        "    \"encl\" : \"enclosed\",\n",
        "    \"encl.\" : \"enclosed\",\n",
        "    \"etc\" : \"and so on\",\n",
        "    \"faq\" : \"frequently asked questions\",\n",
        "    \"fawc\" : \"for anyone who cares\",\n",
        "    \"fb\" : \"facebook\",\n",
        "    \"fc\" : \"fingers crossed\",\n",
        "    \"fig\" : \"figure\",\n",
        "    \"fimh\" : \"forever in my heart\",\n",
        "    \"ft.\" : \"feet\",\n",
        "    \"ft\" : \"featuring\",\n",
        "    \"ftl\" : \"for the loss\",\n",
        "    \"ftw\" : \"for the win\",\n",
        "    \"fwiw\" : \"for what it is worth\",\n",
        "    \"fyi\" : \"for your information\",\n",
        "    \"g9\" : \"genius\",\n",
        "    \"gahoy\" : \"get a hold of yourself\",\n",
        "    \"gal\" : \"get a life\",\n",
        "    \"gcse\" : \"general certificate of secondary education\",\n",
        "    \"gfn\" : \"gone for now\",\n",
        "    \"gg\" : \"good game\",\n",
        "    \"gl\" : \"good luck\",\n",
        "    \"glhf\" : \"good luck have fun\",\n",
        "    \"gmt\" : \"greenwich mean time\",\n",
        "    \"gmta\" : \"great minds think alike\",\n",
        "    \"gn\" : \"good night\",\n",
        "    \"g.o.a.t\" : \"greatest of all time\",\n",
        "    \"goat\" : \"greatest of all time\",\n",
        "    \"goi\" : \"get over it\",\n",
        "    \"gps\" : \"global positioning system\",\n",
        "    \"gr8\" : \"great\",\n",
        "    \"gratz\" : \"congratulations\",\n",
        "    \"gyal\" : \"girl\",\n",
        "    \"h&c\" : \"hot and cold\",\n",
        "    \"hp\" : \"horsepower\",\n",
        "    \"hr\" : \"hour\",\n",
        "    \"hrh\" : \"his royal highness\",\n",
        "    \"ht\" : \"height\",\n",
        "    \"ibrb\" : \"i will be right back\",\n",
        "    \"ic\" : \"i see\",\n",
        "     \"icq\" : \"i seek you\",\n",
        "    \"icymi\" : \"in case you missed it\",\n",
        "    \"idc\" : \"i do not care\",\n",
        "    \"idgadf\" : \"i do not give a damn fuck\",\n",
        "    \"idgaf\" : \"i do not give a fuck\",\n",
        "    \"idk\" : \"i do not know\",\n",
        "    \"ie\" : \"that is\",\n",
        "    \"i.e\" : \"that is\",\n",
        "    \"ifyp\" : \"i feel your pain\",\n",
        "    \"IG\" : \"instagram\",\n",
        "    \"iirc\" : \"if i remember correctly\",\n",
        "    \"ilu\" : \"i love you\",\n",
        "    \"ily\" : \"i love you\",\n",
        "    \"imho\" : \"in my humble opinion\",\n",
        "    \"imo\" : \"in my opinion\",\n",
        "    \"imu\" : \"i miss you\",\n",
        "    \"iow\" : \"in other words\",\n",
        "    \"irl\" : \"in real life\",\n",
        "    \"j4f\" : \"just for fun\",\n",
        "    \"jic\" : \"just in case\",\n",
        "    \"jk\" : \"just kidding\",\n",
        "    \"jsyk\" : \"just so you know\",\n",
        "    \"l8r\" : \"later\",\n",
        "    \"lb\" : \"pound\",\n",
        "    \"lbs\" : \"pounds\",\n",
        "    \"ldr\" : \"long distance relationship\",\n",
        "    \"lmao\" : \"laugh my ass off\",\n",
        "    \"lmfao\" : \"laugh my fucking ass off\",\n",
        "    \"lol\" : \"laughing out loud\",\n",
        "    \"ltd\" : \"limited\",\n",
        "    \"ltns\" : \"long time no see\",\n",
        "    \"m8\" : \"mate\",\n",
        "    \"mf\" : \"motherfucker\",\n",
        "    \"mfs\" : \"motherfuckers\",\n",
        "    \"mfw\" : \"my face when\",\n",
        "    \"mofo\" : \"motherfucker\",\n",
        "    \"mph\" : \"miles per hour\",\n",
        "    \"mr\" : \"mister\",\n",
        "    \"mrw\" : \"my reaction when\",\n",
        "    \"ms\" : \"miss\",\n",
        "    \"mte\" : \"my thoughts exactly\",\n",
        "    \"nagi\" : \"not a good idea\",\n",
        "    \"nbc\" : \"national broadcasting company\",\n",
        "    \"nbd\" : \"not big deal\",\n",
        "    \"nfs\" : \"not for sale\",\n",
        "    \"ngl\" : \"not going to lie\",\n",
        "    \"nhs\" : \"national health service\",\n",
        "    \"nrn\" : \"no reply necessary\",\n",
        "    \"nsfl\" : \"not safe for life\",\n",
        "    \"nsfw\" : \"not safe for work\",\n",
        "    \"nth\" : \"nice to have\",\n",
        "    \"nvr\" : \"never\",\n",
        "    \"nyc\" : \"new york city\",\n",
        "    \"oc\" : \"original content\",\n",
        "    \"og\" : \"original\",\n",
        "    \"ohp\" : \"overhead projector\",\n",
        "    \"oic\" : \"oh i see\",\n",
        "    \"omdb\" : \"over my dead body\",\n",
        "    \"omg\" : \"oh my god\",\n",
        "    \"omw\" : \"on my way\",\n",
        "    \"p.a\" : \"per annum\",\n",
        "    \"p.m\" : \"after midday\",\n",
        "    \"pm\" : \"prime minister\",\n",
        "    \"poc\" : \"people of color\",\n",
        "    \"pov\" : \"point of view\",\n",
        "    \"pp\" : \"pages\",\n",
        "    \"ppl\" : \"people\",\n",
        "    \"prw\" : \"parents are watching\",\n",
        "    \"ps\" : \"postscript\",\n",
        "    \"pt\" : \"point\",\n",
        "    \"ptb\" : \"please text back\",\n",
        "    \"pto\" : \"please turn over\",\n",
        "    \"qpsa\" : \"what happens\",\n",
        "    \"ratchet\" : \"rude\",\n",
        "    \"rbtl\" : \"read between the lines\",\n",
        "    \"rlrt\" : \"real life retweet\",\n",
        "    \"rofl\" : \"rolling on the floor laughing\",\n",
        "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\" : \"retweet\",\n",
        "    \"ruok\" : \"are you ok\",\n",
        "    \"sfw\" : \"safe for work\",\n",
        "    \"sk8\" : \"skate\",\n",
        "    \"smh\" : \"shake my head\",\n",
        "    \"sq\" : \"square\",\n",
        "    \"srsly\" : \"seriously\",\n",
        "    \"ssdd\" : \"same stuff different day\",\n",
        "    \"tbh\" : \"to be honest\",\n",
        "    \"tbs\" : \"tablespooful\",\n",
        "    \"tbsp\" : \"tablespooful\",\n",
        "    \"tfw\" : \"that feeling when\",\n",
        "    \"thks\" : \"thank you\",\n",
        "    \"tho\" : \"though\",\n",
        "    \"thx\" : \"thank you\",\n",
        "    \"tia\" : \"thanks in advance\",\n",
        "    \"til\" : \"today i learned\",\n",
        "    \"tl;dr\" : \"too long i did not read\",\n",
        "    \"tldr\" : \"too long i did not read\",\n",
        "    \"tmb\" : \"tweet me back\",\n",
        "    \"tntl\" : \"trying not to laugh\",\n",
        "    \"ttyl\" : \"talk to you later\",\n",
        "    \"u\" : \"you\",\n",
        "    \"u2\" : \"you too\",\n",
        "    \"u4e\" : \"yours for ever\",\n",
        "    \"utc\" : \"coordinated universal time\",\n",
        "    \"w/\" : \"with\",\n",
        "    \"w/o\" : \"without\",\n",
        "    \"w8\" : \"wait\",\n",
        "    \"wassup\" : \"what is up\",\n",
        "    \"wb\" : \"welcome back\",\n",
        "    \"wtg\" : \"way to go\",\n",
        "    \"wtpa\" : \"where the party at\",\n",
        "    \"wuf\" : \"where are you from\",\n",
        "    \"wuzup\" : \"what is up\",\n",
        "    \"wywh\" : \"wish you were here\",\n",
        "    \"yd\" : \"yard\",\n",
        "    \"ygtr\" : \"you got that right\",\n",
        "    \"ynk\" : \"you never know\",\n",
        "    \"zzz\" : \"sleeping bored and tired\"\n",
        "  }\n",
        "  for i, review in enumerate(reviews):\n",
        "    for j, word in enumerate(review.split()):\n",
        "      if word.lower() in abbreviations.keys():\n",
        "        reviews[i].replace(word, abbreviations[word.lower()])\n",
        "  return reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyJOsNfiPDXO"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data):\n",
        "  #data = expand_data(data)\n",
        "  data = slang_transformer(data)\n",
        "  #data = spelling_corrector(data)\n",
        "  #stopwords = nltk.corpus.stopwords.words('english')\n",
        "  nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma')\n",
        "  processed_data = []\n",
        "  for i in range(0, len(data)):\n",
        "    #doc = re.sub('[^a-zA-Z]', ' ', response[i])\n",
        "    doc = data[i]\n",
        "    doc = nlp(doc)\n",
        "    doc = [str(word.lemma).lower() for sent in doc.sentences for word in sent.words]\n",
        "    doc = ' '.join(doc)\n",
        "    processed_data.append(doc)\n",
        "  return processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-QSfJUfw5HX"
      },
      "outputs": [],
      "source": [
        "def split_train_val_test(X, y, train_ratio, test_ratio, random_state = None):\n",
        "  train_ds, test_ds, train_labels, test_labels = train_test_split(X, y, test_size=test_ratio, random_state=42, shuffle = True, stratify = y)\n",
        "  val_ratio = 1- (train_ratio + test_ratio)\n",
        "  train_ds, val_ds,  train_labels , val_labels = train_test_split(train_ds, train_labels, test_size= val_ratio, random_state=42, stratify = train_labels)\n",
        "  return train_ds, train_labels, val_ds, val_labels, test_ds, test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU4aLfAPwx3j"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ji-urJFP93g"
      },
      "outputs": [],
      "source": [
        "def calculate_idf(processed_data):\n",
        "    count_vectorizer = CountVectorizer(tokenizer=whitespace_tokenizer, token_pattern = None, lowercase=False)\n",
        "    count_vectorizer.fit(processed_data)\n",
        "    tf_matrix = count_vectorizer.transform(processed_data)\n",
        "    doc_freq = np.array(tf_matrix.astype(bool).sum(axis=0)).flatten()\n",
        "    idf = np.log(len(processed_data) / (doc_freq))\n",
        "    return idf.tolist(), count_vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYYGzWb5HXAV"
      },
      "outputs": [],
      "source": [
        "def unique(list1):\n",
        "  unique_list = []\n",
        "  for x in list1:\n",
        "      if x not in unique_list:\n",
        "          unique_list.append(x)\n",
        "  return unique_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srpEgJH57AaM"
      },
      "outputs": [],
      "source": [
        "def get_pos_weights(POS_tags, weights = [1, 1, 0.5]):\n",
        "  weights = []\n",
        "  for sentence_tags in POS_tags:\n",
        "    sentence_weights = []\n",
        "    for tag in sentence_tags:\n",
        "      if(tag == \"VERB\"):\n",
        "        sentence_weights.append(weights[0])\n",
        "      elif(tag == \"NOUN\"):\n",
        "        sentence_weights.append(weights[1])\n",
        "      else:\n",
        "        sentence_weights.append(weights[2])\n",
        "    weights.append(sentence_weights)\n",
        "  return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxXULwRTQrIP"
      },
      "outputs": [],
      "source": [
        "def get_idf_weights(preprocessed_reviews):\n",
        "  idf_vectors, vectorizer = calculate_idf(preprocessed_reviews)\n",
        "  weights = {}\n",
        "  for name, vector in zip(vectorizer.get_feature_names_out(), idf_vectors):\n",
        "    weights.update({name : vector})\n",
        "  return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GHCGgxwxMJg"
      },
      "source": [
        "# Deep Learning Models Training and Evaluation Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEbScrauHdhu"
      },
      "outputs": [],
      "source": [
        "def compile_model(classifier_model, loss = tf.keras.losses.CategoricalCrossentropy(), metrics = [tf.metrics.Recall(), tf.metrics.Precision()]):\n",
        "  classifier_model = build_classifier_model()\n",
        "  classifier_model.compile(optimizer=\"adam\",\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "  return classifier_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeK2aZe1HepW"
      },
      "outputs": [],
      "source": [
        "def prepare_callbacks(names = ['early_stop', 'reduceLR']):\n",
        "  callbacks = []\n",
        "  if('early_stop' in names):\n",
        "   earlyStopping = tf.keras.callbacks.EarlyStopping(\n",
        "     monitor=\"val_loss\",\n",
        "     min_delta = 0.0001,\n",
        "     patience = 20,\n",
        "     verbose=1,\n",
        "     mode=\"min\",\n",
        "     restore_best_weights=True)\n",
        "   callbacks.append(earlyStopping)\n",
        "  if('reduceLR' in names):\n",
        "   reduceLR = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "       monitor=\"val_loss\",\n",
        "       factor=0.5,\n",
        "       patience=10,\n",
        "       verbose=1,\n",
        "       mode=\"min\",\n",
        "       min_delta=0.0001,\n",
        "       cooldown=0,\n",
        "       min_lr=0)\n",
        "   callbacks.append(reduceLR)\n",
        "  return callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLg1wPwOHgy_"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_ds, tr_labels, val_ds, val_labels, epochs = 50, batch_size = 32, callbacks = []):\n",
        "  history = model.fit(x= tf.constant(train_ds),\n",
        "                               y = tf.constant(tr_labels),\n",
        "                               validation_data = (val_ds, tf.constant(val_labels)),\n",
        "                               epochs=epochs,batch_size=batch_size, callbacks = callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCST6elB25i0"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(classifier_model, test_data, test_labels):\n",
        "  classifier_model.evaluate(test_data, tf.constant(test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTV_ie6L38zc"
      },
      "source": [
        "# Deep Learning Models Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewkPam4239bu"
      },
      "outputs": [],
      "source": [
        "def build_NN_model(shape = shape):\n",
        "  input = tf.keras.layers.Input(shape=(shape))\n",
        "  net = tf.keras.layers.Dropout(0.1)(input)\n",
        "  net = tf.keras.layers.Dense(len(classes), activation='softmax', name='classifier')(net)\n",
        "  return tf.keras.Model(input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzWnmzjU30rb"
      },
      "outputs": [],
      "source": [
        "def build_finetuned_model(trainable = False):\n",
        "  input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='sentences')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable = trainable, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  embedding_vectors = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(embedding_vectors)\n",
        "  net = tf.keras.layers.Dense(len(classes), activation='softmax', name='classifier')(net)\n",
        "  return tf.keras.Model(input, net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5rlnjKFz1si"
      },
      "source": [
        "# Get Text Representaions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLSKfqFA8a9U"
      },
      "outputs": [],
      "source": [
        "def get_fastText_model(model_type = \"domain\"):\n",
        "  if (model_type == \"domain\" ):\n",
        "    ft_model = fasttext.load_model(\"/gdrive/MyDrive/Project/train_100_single_epoch50.bin\")\n",
        "  elif (model_type = \"300_dim\"):\n",
        "    ft_model = fasttext.load_model('cc.en.300.bin')\n",
        "  return ft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2JT3kRkF6xL"
      },
      "outputs": [],
      "source": [
        "def get_fastText_embedding(data):\n",
        "  embeddings = np.zeros(shape=(len(data), ft_model.get_dimension()), dtype = 'float32')\n",
        "  for i, review in enumerate(data):\n",
        "    review_embedding = np.zeros(shape=(ft_model.get_dimension(),), dtype = 'float32')\n",
        "    words_count = 0\n",
        "    for word in review.lower().split():\n",
        "      words_count = words_count + 1\n",
        "      word_embedding = ft_model.get_word_vector(word).astype('float32')\n",
        "      review_embedding = review_embedding + word_embedding\n",
        "    review_embedding = review_embedding/words_count\n",
        "    embeddings[i] = review_embedding\n",
        "  return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4MgbYiwPj5I"
      },
      "outputs": [],
      "source": [
        "def get_idf_weighted_fastText_embedding(data, weights):\n",
        "  embeddings = np.zeros(shape=(len(data), ft_model.get_dimension()), dtype = 'float32')\n",
        "  for i, review in enumerate(data):\n",
        "    review_embedding = np.zeros(shape=(ft_model.get_dimension(),), dtype = 'float32')\n",
        "    words_count = 0\n",
        "    for word in whitespace_tokenizer(review):\n",
        "      words_count = words_count + 1\n",
        "      word_embedding = ft_model.get_word_vector(word).astype('float32') * weights[word]\n",
        "      review_embedding = review_embedding + word_embedding\n",
        "    if(words_count == 0):\n",
        "      continue\n",
        "    review_embedding = review_embedding/words_count\n",
        "    embeddings[i] = review_embedding\n",
        "  return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX5zToToDjgg"
      },
      "outputs": [],
      "source": [
        "def get_pos_weighted_fastText_embedding(data, weights):\n",
        "  embeddings = np.zeros(shape=(len(data), ft_model.get_dimension()), dtype = 'float32')\n",
        "  for i, review in enumerate(data):\n",
        "    review_embedding = np.zeros(shape=(ft_model.get_dimension(),), dtype = 'float32')\n",
        "    words_count = 0\n",
        "    for j, word in enumerate(review.split()):\n",
        "      if(weights[i][j] == 0):\n",
        "        continue\n",
        "      words_count = words_count + 1\n",
        "      word_embedding = ft_model.get_word_vector(word).astype('float32') * weights[i][j]\n",
        "      review_embedding = review_embedding + word_embedding\n",
        "    if(words_count != 0):\n",
        "      review_embedding = review_embedding/words_count\n",
        "    embeddings[i] = review_embedding\n",
        "  return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay-u67Aq0zuZ"
      },
      "outputs": [],
      "source": [
        "def get_pos_concatenated_fastText_embedding(data, pos_tags):\n",
        "  embeddings = np.zeros(shape=(len(data), ft_model.get_dimension() * 2), dtype = 'float32')\n",
        "  for i, review in enumerate(data):\n",
        "    nouns_review_embedding = np.zeros(shape=(ft_model.get_dimension()), dtype = 'float32')\n",
        "    verbs_review_embedding = np.zeros(shape=(ft_model.get_dimension()), dtype = 'float32')\n",
        "    nouns_words_count = 0\n",
        "    verbs_words_count = 0\n",
        "    for j, word in enumerate(review.split()):\n",
        "      if(pos_tags[i][j] == \"NOUN\"):\n",
        "        nouns_words_count = nouns_words_count + 1\n",
        "        word_embedding = ft_model.get_word_vector(word).astype('float32')\n",
        "        nouns_review_embedding = nouns_review_embedding + word_embedding\n",
        "      elif(pos_tags[i][j] == \"VERB\"):\n",
        "        verbs_words_count = verbs_words_count + 1\n",
        "        word_embedding = ft_model.get_word_vector(word).astype('float32')\n",
        "        verbs_review_embedding = verbs_review_embedding + word_embedding\n",
        "    if(nouns_words_count != 0):\n",
        "      nouns_review_embedding = nouns_review_embedding/nouns_words_count\n",
        "    if(verbs_words_count != 0):\n",
        "      verbs_review_embedding = verbs_review_embedding/verbs_words_count\n",
        "    embeddings[i] = np.concatenate([nouns_review_embedding, verbs_review_embedding])\n",
        "  return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaaWoHGFlLWK"
      },
      "outputs": [],
      "source": [
        "def get_BoW_vectors(processed_data):\n",
        "  CountVec = CountVectorizer(ngram_range=(1,1))\n",
        "  vectors = CountVec.fit_transform(processed_data)\n",
        "  vectors = vectors.toarray()\n",
        "  for i, vector in enumerate(vectors):\n",
        "   for j, elem in enumerate(vector):\n",
        "     if(vectors[i][j]> 1):\n",
        "       vectors[i][j] = 1\n",
        "  return vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGSlmaTO7CYP"
      },
      "outputs": [],
      "source": [
        "def get_TF_vectors(processed_data):\n",
        "  CountVec = CountVectorizer(ngram_range=(1,1))\n",
        "  vectors = CountVec.fit_transform(processed_data)\n",
        "  vectors = vectors.toarray()\n",
        "  return vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3MBkrOW7LQ5"
      },
      "outputs": [],
      "source": [
        "def get_idf_vectors(processed_data):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  vectors = vectorizer.fit_transform(processed_data)\n",
        "  vectors = vectors.toarray()\n",
        "  return vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YihplgEiGTIL"
      },
      "outputs": [],
      "source": [
        "def prepare_BERT_embedding_model():\n",
        "  bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "  map_name_to_handle = {\n",
        "      'bert_en_uncased_L-12_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "      'bert_en_cased_L-12_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "      'bert_multi_cased_L-12_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "      'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "      'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "      'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "      'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "      'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "      'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "      'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "      'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "      'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "      'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "      'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "      'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "      'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "      'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "      'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "      'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "      'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "      'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "      'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "      'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "      'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "      'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "      'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "      'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "      'albert_en_base':\n",
        "          'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "      'electra_small':\n",
        "          'https://tfhub.dev/google/electra_small/2',\n",
        "      'electra_base':\n",
        "          'https://tfhub.dev/google/electra_base/2',\n",
        "      'experts_pubmed':\n",
        "          'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "      'experts_wiki_books':\n",
        "          'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "      'talking-heads_base':\n",
        "          'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "  }\n",
        "  map_model_to_preprocess = {\n",
        "      'bert_en_uncased_L-12_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'bert_en_cased_L-12_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'bert_multi_cased_L-12_H-768_A-12':\n",
        "          'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "      'albert_en_base':\n",
        "          'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "      'electra_small':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'electra_base':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'experts_pubmed':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'experts_wiki_books':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "      'talking-heads_base':\n",
        "          'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "  }\n",
        "  tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "  tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "  print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "  print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n",
        "  return tfhub_handle_encoder, tfhub_handle_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPYEPgf56v9S"
      },
      "outputs": [],
      "source": [
        "def get_BERT_embeddings(data):\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(data)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=False, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  embedding_vectors = outputs['pooled_output']\n",
        "  return embedding_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gGBjzOf2C5D"
      },
      "source": [
        "# Train and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgQBH1rhSHtE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3phvXFj2CRO"
      },
      "outputs": [],
      "source": [
        "# change dataset_name, features_list, target according to the dataset\n",
        "dataset_name = \"P1-Golden.xlsx\"\n",
        "projected_columns_list = [\"Reviews\", \"Judgment\"]\n",
        "target = \"Judgment\"\n",
        "features_list = [\"Reviews\"]\n",
        "dataset = read_data(dataset_name, projected_columns_list)\n",
        "classes = unique(dataset[target].tolist())\n",
        "reviews, labels = prepare_data(dataset, features_list, target , classes)\n",
        "\n",
        "# Fast Text Representaions\n",
        "# 300 dim\n",
        "ft_model = get_fastText_model(model_type = \"300_dim\")\n",
        "\n",
        "# domain\n",
        "ft_model = get_fastText_model(model_type = \"domain\")\n",
        "\n",
        "  # Average\n",
        "  reviews = get_fastText_embedding(reviews)\n",
        "\n",
        "  # idf-weighted\n",
        "  weights = get_idf_weights(reviews)\n",
        "  reviews = get_idf_weighted_fastText_embedding(reviews, weights)\n",
        "\n",
        "  # pos-weighted\n",
        "  pos_tags = get_pos_tags(reviews)\n",
        "  reviews = tokenize_data(reviews)\n",
        "  weights = get_pos_weights(pos_tags)\n",
        "  reviews = get_pos_weighted_fastText_embedding(reviews, weights)\n",
        "\n",
        "  # concat\n",
        "  pos_tags = get_pos_tags(reviews)\n",
        "  reviews = tokenize_data(reviews)\n",
        "  reviews = get_pos_concatenated_fastText_embedding(reviews, pos_tags)\n",
        "\n",
        "# BoW-binary Representaion\n",
        "processed_data = preprocess_date(reviews)\n",
        "reviews = get_BOW_vectors(processed_data)\n",
        "\n",
        "# TF Representaion\n",
        "processed_data = preprocess_date(reviews)\n",
        "reviews = get_tf_vectors(processed_data)\n",
        "\n",
        "# Tf-IDF Representaion\n",
        "processed_data = preprocess_date(reviews)\n",
        "reviews = get_idf_vectors(processed_data)\n",
        "\n",
        "# BERT CLS Representaion\n",
        "tfhub_handle_encoder, tfhub_handle_preprocess = prepare_BERT_embedding_model()\n",
        "reviews = get_BERT_embeddings(reviews)\n",
        "\n",
        "# Fine tune BERT\n",
        "model = build_finetuned_model(trainble = True)\n",
        "\n",
        "train_ds, train_labels, val_ds, val_labels, test_ds, test_labels = split_train_val_test(reviews, labels, train_ratio = 0.7, test_ratio = 0.15, random_state = 42)\n",
        "train_labels = get_one_hot_vectors(train_labels)\n",
        "one_test_labels = get_one_hot_vectors(test_labels)\n",
        "val_labels = get_one_hot_vectors(val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbBcxXAVSjDX"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoGPdDbYPklv"
      },
      "outputs": [],
      "source": [
        "# NN Model\n",
        "model = build_classifier_model()\n",
        "model = compile_model(model)\n",
        "callbacks = prepare_callbacks()\n",
        "train_model(model, train_ds, train_labels,val_ds, val_labels, epochs = 1000, batch_size = 32, callbacks = callbacks)\n",
        "\n",
        "# Fine-tuned Model\n",
        "train_model(model, train_ds, train_labels,val_ds, val_labels, epochs = 1000, batch_size = 32, callbacks = callbacks)\n",
        "\n",
        "# SVM without hyperparameters tuning\n",
        "model = SVC(C = 0.1, coef0 = 10,  degree = 2, gamma = 0.1, kernel = 'poly', random_state = 42) # change these parameters as you want\n",
        "model.fit(train_ds, train_labels)\n",
        "\n",
        "# SVM with hyperparameters tuning\n",
        "svm = SVC(random_state=  42)\n",
        "model = GridSearchCV(svm, param_grid={ 'gamma' : [0.001, 0.01, 0.1, 1, 10], 'C' : [0.01, 0.1, 1, 10, 100],\n",
        "                                   'kernel':['poly', 'rbf', 'linear'], 'degree': [2, 3, 4], 'coef0' : [0, 0.1, 1.0, 10]},\n",
        "                      cv= skf, scoring = 'f1_weighted', verbose = 2)\n",
        "model.fit(np.array(train_ds),train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYNmaT0YSmED"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzRW9WLNQl_m"
      },
      "outputs": [],
      "source": [
        "# NN Model\n",
        "# Fine-tuned Model\n",
        "evaluate_model(model, test_ds, get_one_hot_vectors(test_labels))\n",
        "predictions = model.predict(test_ds)\n",
        "prediction_labels = []\n",
        "for prediction in predictions:\n",
        "  prediction_labels.append(np.argmax(prediction))\n",
        "\n",
        "# SVM\n",
        "# NN Model\n",
        "# Fine-tuned Model\n",
        "print(classification_report(test_labels, prediction_labels))\n",
        "\n",
        "# SVN with Grid Search\n",
        "print(classification_report(test_labels, gd.predict(np.array(test_ds))))\n",
        "print(gd.best_params_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jAyr552N8A5U",
        "GsDyh0iJ77n_",
        "WIjGivHnwd71",
        "cU4aLfAPwx3j",
        "3GHCGgxwxMJg",
        "eTV_ie6L38zc",
        "v5rlnjKFz1si"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
